{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3.dynamodb.conditions import Attr\n",
    "res = (Attr('test').eq(1) & Attr('a').lt(2)) | Attr('b').contains('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"(( A__eq:str:1)OR(B__lt:str:2)AND(C__gt:int:3)AND(D:str:a))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(',\n",
       " '(',\n",
       " 'A__eq',\n",
       " 'str',\n",
       " '1',\n",
       " ')',\n",
       " 'OR',\n",
       " '(',\n",
       " 'B__lt',\n",
       " 'str',\n",
       " '2',\n",
       " ')',\n",
       " 'AND',\n",
       " '(',\n",
       " 'C__gt',\n",
       " 'int',\n",
       " '3',\n",
       " ')',\n",
       " 'AND',\n",
       " '(',\n",
       " 'D',\n",
       " 'str',\n",
       " 'a',\n",
       " ')',\n",
       " ')']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1\n",
    "# Split string into a list of untyped tokens\n",
    "import re\n",
    "separators = ['\\(', '\\)', ':']\n",
    "\n",
    "# re.partition(re.split(''))\n",
    "rstr = \"({})\".format('|'.join(separators))\n",
    "splitted = re.split(re.compile(rstr), test)\n",
    "tokens_to_ignore = {':', ''}\n",
    "no_type_tokens = list(filter(lambda tok: tok not in tokens_to_ignore, [s.strip() for s in splitted]))\n",
    "no_type_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get token primary and secondary types\n",
    "\n",
    "\n",
    "class Token:\n",
    "    def __init__(self, literal, primary_type, secondary_type):\n",
    "        if primary_type not in registered_token_types or secondary_type not in registered_token_types[primary_type]:\n",
    "            raise Exception('Attempted to tokenize an unregistered primary/secondary type')\n",
    "        \n",
    "        self.literal = literal\n",
    "        self.primary_type = primary_type\n",
    "        self.secondary_type = secondary_type\n",
    "        self.string_notation = f'{primary_type}:{secondary_type}'\n",
    "    \n",
    "    def set_string_notation(self):\n",
    "        self.string_notation = f'{primary_type}:{secondary_type}'\n",
    "    \n",
    "    def update_secondary_type(self, new_secondary_type):\n",
    "        self.secondary_type = new_secondary_type\n",
    "        self.set_string_notation()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.string_notation\n",
    "\n",
    "\n",
    "class ContextToken(Token):\n",
    "    pass\n",
    "\n",
    "\n",
    "import operator\n",
    "token_operator = {'and': operator.and_, 'or': operator.or_}\n",
    "token_precedence = {'and': 2, 'or': 1}\n",
    "\n",
    "class LogicalToken(Token):\n",
    "    def __init__(self, literal, primary_type, secondary_type):\n",
    "        super().__init__(literal, primary_type, secondary_type)\n",
    "        self.precedence = token_precedence[self.secondary_type]\n",
    "        self.operator = token_operator[self.secondary_type]\n",
    "\n",
    "from decimal import Decimal\n",
    "from datetime import datetime, date\n",
    "value_types = {\n",
    "    'int': int,\n",
    "    'str': str,\n",
    "    'float': float,\n",
    "    'decimal': Decimal,\n",
    "    'date': date,\n",
    "    'datetime': datetime\n",
    "}\n",
    "    \n",
    "class ExpressionToken(Token):\n",
    "    def get_field_and_lookup_expression(self):\n",
    "        if not self.secondary_type == 'field':\n",
    "            return None\n",
    "        \n",
    "        splitted_field = self.literal.split('__')\n",
    "        splitted_count = len(splitted_field)\n",
    "        if splitted_count == 1:\n",
    "            lookup_expression = 'eq'\n",
    "        elif splitted_count in [2, 3]:\n",
    "            lookup_expression = splitted_field[-1]\n",
    "        else:\n",
    "            raise Exception('Invalid lookup expression!')\n",
    "        \n",
    "        return splitted_field[0], lookup_expression\n",
    "    \n",
    "    def get_value_type_casting_method(self):\n",
    "        if not self.secondary_type == 'value_type':\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            return value_types[self.literal]\n",
    "        except KeyError:\n",
    "            raise Exception('Invalid value type!')\n",
    "        \n",
    "\n",
    "\n",
    "registered_token_types = {\n",
    "    'context': {'open', 'close'},\n",
    "    'logical': {'and', 'or'},\n",
    "    'expression': {'field', 'value_type', 'value'}\n",
    "}\n",
    "\n",
    "general_token_types = {\n",
    "    '(': ('context', 'open'),\n",
    "    ')': ('context', 'close'),\n",
    "    'AND': ('logical', 'and'),\n",
    "    'OR': ('logical', 'or')\n",
    "}\n",
    "\n",
    "field_lambda = lambda _: ('expression', 'field')\n",
    "expression_token_types = {\n",
    "    # previous token: (typ, subtype)\n",
    "    None: field_lambda,\n",
    "    'context:open': field_lambda,\n",
    "    'expression:field': lambda next_literal: ('expression', 'value') if next_literal in [None, ')'] else ('expression', 'value_type'),\n",
    "    'expression:value_type': lambda _: ('expression', 'value')\n",
    "}\n",
    "\n",
    "primary_type_to_Token = {\n",
    "    'context': ContextToken,\n",
    "    'logical': LogicalToken,\n",
    "    'expression': ExpressionToken\n",
    "}\n",
    "\n",
    "def get_instanced_token(current_literal: str, previous_token_string_notation: str, next_literal: str):\n",
    "    try:\n",
    "        primary_type, secondary_type = general_token_types[current_literal]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            primary_type, secondary_type = expression_token_types[previous_token_string_notation](next_literal)\n",
    "        except KeyError:\n",
    "            raise Exception(f'Couldn\\'t determine token type!\\nCurrent -> {current_literal}\\nPrevious -> {previous_token_string_notation}\\nNext -> {next_literal}')\n",
    "    \n",
    "    if primary_type == None or secondary_type == None:\n",
    "        return None\n",
    "    \n",
    "    return primary_type_to_Token[primary_type](current_literal, primary_type, secondary_type)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(( A__eq:str:1)OR(B__lt:str:2)AND(C__gt:int:3)AND(D:str:a))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['context:open -> (',\n",
       " 'context:open -> (',\n",
       " 'expression:field -> A__eq',\n",
       " 'expression:value_type -> str',\n",
       " 'expression:value -> 1',\n",
       " 'context:close -> )',\n",
       " 'logical:or -> OR',\n",
       " 'context:open -> (',\n",
       " 'expression:field -> B__lt',\n",
       " 'expression:value_type -> str',\n",
       " 'expression:value -> 2',\n",
       " 'context:close -> )',\n",
       " 'logical:and -> AND',\n",
       " 'context:open -> (',\n",
       " 'expression:field -> C__gt',\n",
       " 'expression:value_type -> int',\n",
       " 'expression:value -> 3',\n",
       " 'context:close -> )',\n",
       " 'logical:and -> AND',\n",
       " 'context:open -> (',\n",
       " 'expression:field -> D',\n",
       " 'expression:value_type -> str',\n",
       " 'expression:value -> a',\n",
       " 'context:close -> )',\n",
       " 'context:close -> )']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instanced_tokens = []\n",
    "\n",
    "previous_token = None\n",
    "for index, current_token in enumerate(no_type_tokens):\n",
    "\n",
    "    try:\n",
    "        next_token = no_type_tokens[index + 1]\n",
    "    except IndexError:\n",
    "        next_token = None\n",
    "\n",
    "    instanced_token = get_instanced_token(current_token, previous_token, next_token)\n",
    "    instanced_tokens.append(instanced_token)\n",
    "    previous_token = str(instanced_token)\n",
    "\n",
    "print(test)\n",
    "[f'{str(t)} -> {t.literal}' for t in instanced_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(( A__eq:str:1)OR(B__lt:str:2)AND(C__gt:int:3)AND(D:str:a))\n",
      "{'operator': <built-in function or_>, 'left_child': <__main__.ExpressionNode object at 0x7f15406e0080>, 'right_child': <__main__.LogicalNode object at 0x7f15406c9b38>, '_uid': 2}\n",
      "{'field': 'A', 'lookup_expression': 'eq', 'value': '1', 'left_child': None, 'right_child': None, '_uid': 1}\n",
      "{'field': 'B', 'lookup_expression': 'lt', 'value': '2', 'left_child': None, 'right_child': None, '_uid': 3}\n",
      "{'field': 'C', 'lookup_expression': 'gt', 'value': 3, 'left_child': None, 'right_child': None, '_uid': 5}\n",
      "{'field': 'D', 'lookup_expression': 'eq', 'value': 'a', 'left_child': None, 'right_child': None, '_uid': 7}\n"
     ]
    }
   ],
   "source": [
    "class Node:\n",
    "    uid = 0\n",
    "    def __init__(self, left_child=None, right_child=None):\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        Node.uid +=1\n",
    "        self._uid = Node.uid\n",
    "\n",
    "class ExpressionNode(Node):\n",
    "    def __init__(self, *args):\n",
    "        \n",
    "        if len(args) not in [2, 3]:\n",
    "            raise Exception(f'Couldn\\'t build ExpressionNode! Tokens -> {args}')\n",
    "        \n",
    "        field_token = args[0]\n",
    "        value_token = args[-1]\n",
    "        value_type_token = args[1] if len(args) == 3 else None\n",
    "            \n",
    "        if any(not isinstance(tok, ExpressionToken) for tok in [field_token, value_token]):\n",
    "            raise Exception('Cannot build expression node out of non ExpressionTokens!')\n",
    "        \n",
    "        field, lookup_expression = field_token.get_field_and_lookup_expression()\n",
    "        self.field = field\n",
    "        self.lookup_expression = lookup_expression\n",
    "        \n",
    "#         import ipdb; ipdb.set_trace\n",
    "        value_type = None if not value_type_token else value_type_token.get_value_type_casting_method()\n",
    "        self.value = value_type(value_token.literal) if value_type else value_token.literal\n",
    "        super().__init__()\n",
    "        \n",
    "class LogicalNode(Node):\n",
    "    def __init__(self, logical_token):\n",
    "        self.operator = logical_token.operator\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "next_expected_tokens = {\n",
    "    None: {'context:open', 'expression:field'},\n",
    "    'context:open': {'context:open', 'expression:field'},\n",
    "    'context:close': {'context:close', 'logical:and', 'logical:or'},\n",
    "    'logical:and': {'context:open'},\n",
    "    'logical:or': {'context:open'},\n",
    "    'expression:field': {'expression:value_type', 'expression:value'},\n",
    "    'expression:value_type': {'expression:value'},\n",
    "    'expression:value': {'context:close'}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# build AST\n",
    "from copy import deepcopy\n",
    "\n",
    "contexts_stack = [{'head': None, 'last_node': None, 'last_logical': None}]\n",
    "current_expression_tokens = []\n",
    "expected_tokens = next_expected_tokens[None]\n",
    "\n",
    "# instanced_tokens.append(None)  # End\n",
    "for tok in instanced_tokens:\n",
    "    tok_repr = str(tok)\n",
    "    if tok_repr not in expected_tokens:\n",
    "        import pdb; pdb.set_trace()\n",
    "        raise Exception(f'Unexpected token encountered -> {tok_repr}, one of the following was expected -> {expected_tokens}')\n",
    "    \n",
    "    if isinstance(tok, ContextToken):\n",
    "        if tok.secondary_type == 'open':\n",
    "            contexts_stack.append({'head': None, 'last_node': None, 'last_logical': None})\n",
    "        elif tok.secondary_type == 'close':\n",
    "            if not contexts_stack:\n",
    "                raise Exception('Attempted to close unexistent context!')\n",
    "\n",
    "            if current_expression_tokens:\n",
    "                try:\n",
    "                    expression_node = ExpressionNode(*current_expression_tokens)\n",
    "                    current_expression_tokens = []\n",
    "                except Exception as e:\n",
    "                    raise Exception(f'Couldn\\'t parse expression node! -> {e}')\n",
    "\n",
    "                node_to_add = deepcopy(expression_node)\n",
    "                if contexts_stack[-1]['head'] == None:\n",
    "                    contexts_stack[-1]['head'] = node_to_add\n",
    "                elif isinstance(contexts_stack[-1]['last_node'], LogicalNode):\n",
    "                    contexts_stack[-1]['last_node'].right_child = node_to_add\n",
    "                else:\n",
    "                    raise Exception('Found an unexpected expression??')\n",
    "                \n",
    "                expression_node = None\n",
    "                contexts_stack[-1]['last_node'] = node_to_add\n",
    "\n",
    "#             else:\n",
    "#             import ipdb; ipdb.set_trace()\n",
    "            context_to_merge = contexts_stack.pop()\n",
    "            if contexts_stack[-1]['head'] == None:\n",
    "                contexts_stack[-1]['head'] = context_to_merge['head']\n",
    "            elif isinstance(contexts_stack[-1]['last_node'], LogicalNode):\n",
    "                contexts_stack[-1]['last_node'].right_child = context_to_merge['head']\n",
    "            contexts_stack[-1]['last_node'] = context_to_merge['head']\n",
    "    \n",
    "    elif isinstance(tok, LogicalToken):\n",
    "#         if contexts_stack[-1]['last_logical'].precedence <\n",
    "#         import ipdb;ipdb.set_trace()\n",
    "#         print('test')\n",
    "        logical_node = LogicalNode(tok)\n",
    "        try:\n",
    "            logical_node.left_child = contexts_stack[-1]['last_node']\n",
    "        except:\n",
    "            raise Exception('No previous node for logical operator!')\n",
    "        if contexts_stack[-1]['last_node'] == contexts_stack[-1]['head']:\n",
    "            contexts_stack[-1]['head'] = logical_node\n",
    "        else:\n",
    "            contexts_stack[-1]['last_logical'].right_child = logical_node\n",
    "    \n",
    "        contexts_stack[-1]['last_node'] = logical_node\n",
    "        contexts_stack[-1]['last_logical'] = logical_node\n",
    "\n",
    "    elif isinstance(tok, ExpressionToken):\n",
    "        current_expression_tokens.append(tok)            \n",
    "            \n",
    "\n",
    "    expected_tokens = next_expected_tokens[str(tok)]\n",
    "\n",
    "if not len(contexts_stack) == 1:\n",
    "    raise Excpetion('Unexpected end of string')\n",
    "\n",
    "print(test)\n",
    "print(contexts_stack[0]['head'].__dict__)\n",
    "print(contexts_stack[0]['head'].left_child.__dict__)\n",
    "print(contexts_stack[0]['head'].right_child.left_child.__dict__)\n",
    "print(contexts_stack[0]['head'].right_child.right_child.left_child.__dict__)\n",
    "print(contexts_stack[0]['head'].right_child.right_child.right_child.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User1\n",
      "1\n",
      "User2\n",
      "2\n",
      "User3\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NewUser:\n",
    "\n",
    "    uid = 0\n",
    "    def __init__(self, username):\n",
    "        self.username = username\n",
    "        NewUser.uid += 1\n",
    "        self._uid = NewUser.uid\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, NewUser):\n",
    "            raise Exception('Not implemented!')\n",
    "        return self._uid == other._uid\n",
    " \n",
    "x = NewUser(\"User1\")\n",
    "print(x.username)\n",
    "print(x._uid)\n",
    " \n",
    "y = NewUser(\"User2\")\n",
    "print(y.username)\n",
    "print(y._uid)\n",
    " \n",
    "z = NewUser(\"User3\")\n",
    "print(z.username)\n",
    "print(z._uid)\n",
    "\n",
    "x == x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
